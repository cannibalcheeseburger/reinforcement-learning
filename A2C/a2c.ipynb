{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else  \n",
    "    \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.98\n",
    "n_rollout = 30\n",
    "map_size = 8\n",
    "env = gym.make('FrozenLake-v1',map_name = f'{map_size}x{map_size}', is_slippery=False)\n",
    "state_dim = env.observation_space.n\n",
    "action_dim = env.action_space.n\n",
    "max_episodes = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        # Actor network (Ï€)\n",
    "        self.pi_fc1 = nn.Linear(state_dim, 128)\n",
    "        self.pi_fc2 = nn.Linear(128, action_dim)\n",
    "        \n",
    "        # Critic network (V)\n",
    "        self.v_fc1 = nn.Linear(state_dim, 128)\n",
    "        self.v_fc2 = nn.Linear(128, 1)\n",
    "        \n",
    "        # Separate optimizers\n",
    "        self.actor_optim = optim.Adam(self.actor_parameters(), lr=0.001)\n",
    "        self.critic_optim = optim.Adam(self.critic_parameters(), lr=0.002)\n",
    "        self.data = []\n",
    "        \n",
    "    def actor_parameters(self):\n",
    "        return list(self.pi_fc1.parameters()) + list(self.pi_fc2.parameters())\n",
    "    \n",
    "    def critic_parameters(self):\n",
    "        return list(self.v_fc1.parameters()) + list(self.v_fc2.parameters())\n",
    "        \n",
    "    def pi(self, x, softmax_dim=0):\n",
    "        x = F.relu(self.pi_fc1(x))\n",
    "        x = self.pi_fc2(x)\n",
    "        return F.softmax(x, dim=softmax_dim)\n",
    "    \n",
    "    def v(self, x):\n",
    "        x = F.relu(self.v_fc1(x))\n",
    "        return self.v_fc2(x)\n",
    "    \n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    def make_batch(self):\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_lst = [], [], [], [], []\n",
    "        for transition in self.data:\n",
    "            s, a, r, s_prime, done = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r/100.0])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_lst.append([0.0] if done else [1.0])\n",
    "            \n",
    "        return (torch.tensor(s_lst, dtype=torch.float, device=device),\n",
    "                torch.tensor(a_lst, device=device),\n",
    "                torch.tensor(r_lst, dtype=torch.float, device=device),\n",
    "                torch.tensor(s_prime_lst, dtype=torch.float, device=device),\n",
    "                torch.tensor(done_lst, dtype=torch.float, device=device))\n",
    "  \n",
    "    def train_net(self):\n",
    "        s, a, r, s_prime, done = self.make_batch()\n",
    "        td_target = r + gamma * self.v(s_prime) * done\n",
    "        delta = td_target - self.v(s)\n",
    "        \n",
    "        pi = self.pi(s, softmax_dim=1)\n",
    "        pi_a = pi.gather(1, a)\n",
    "        policy_loss = -torch.log(pi_a) * delta.detach()\n",
    "        value_loss = F.mse_loss(self.v(s), td_target.detach())\n",
    "        loss = policy_loss + value_loss\n",
    "\n",
    "        self.actor_optim.zero_grad()\n",
    "        self.critic_optim.zero_grad()\n",
    "\n",
    "        policy_loss.mean().backward()\n",
    "        value_loss.backward()\n",
    "\n",
    "        self.actor_optim.step()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        self.data = []\n",
    "        return policy_loss.mean().detach(),value_loss.detach(),loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(state, state_dim):\n",
    "    vec = np.zeros(state_dim)\n",
    "    vec[state] = 1.0\n",
    "    return torch.tensor(vec, dtype=torch.float, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env):\n",
    "    state_dim = env.observation_space.n\n",
    "    action_dim = env.action_space.n\n",
    "    writer = SummaryWriter(log_dir=f'runs/A2C_{map_size}x{map_size}')\n",
    "\n",
    "    model = ActorCritic(state_dim, action_dim)\n",
    "    model.to(device)\n",
    "    print_interval = 100\n",
    "    score = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    for n_epi in range(max_episodes):\n",
    "        s, _ = env.reset()\n",
    "        s = one_hot(s, state_dim)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            for t in range(n_rollout):\n",
    "                steps+=1\n",
    "                with torch.no_grad():\n",
    "                    prob = model.pi(s)\n",
    "                m = Categorical(prob)\n",
    "                a = m.sample().item()\n",
    "                s_prime, r, terminated, truncated, _ = env.step(a)\n",
    "                done = terminated or truncated\n",
    "                s_prime_oh = one_hot(s_prime, state_dim)\n",
    "                model.put_data((s.cpu().numpy(), a, r, s_prime_oh.cpu().numpy(), done))\n",
    "                s = s_prime_oh\n",
    "                score += r\n",
    "                if done:\n",
    "                    break\n",
    "        actor_loss,critic_loss,loss = model.train_net()\n",
    "\n",
    "        if n_epi % print_interval == 0 and n_epi != 0:\n",
    "            avg_score = score / print_interval\n",
    "            print(f\"Episode: {n_epi}, Avg Score: {avg_score:.2f}\")\n",
    "            \n",
    "            writer.add_scalar('Return', avg_score, steps)\n",
    "            # writer.add_scalar('Actor Loss', actor_loss.mean(), steps)\n",
    "            writer.add_scalar('Critic Loss', critic_loss.item(), steps)\n",
    "\n",
    "            score = 0.0\n",
    "    env.close()\n",
    "    writer.close()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(model):\n",
    "    env = gym.make('FrozenLake-v1', is_slippery=True, map_name = f'{map_size}x{map_size}', render_mode='human')\n",
    "    state_dim = env.observation_space.n\n",
    "    s, _ = env.reset()\n",
    "    s = one_hot(s, state_dim)\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            prob = model.pi(s)\n",
    "        a = torch.argmax(prob).item()\n",
    "        s_prime, r, terminated, truncated, _ = env.step(a)\n",
    "        done = terminated or truncated\n",
    "        s = one_hot(s_prime, state_dim)\n",
    "        env.render()\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100, Avg Score: 0.00\n",
      "Episode: 200, Avg Score: 0.01\n",
      "Episode: 300, Avg Score: 0.00\n",
      "Episode: 400, Avg Score: 0.00\n",
      "Episode: 500, Avg Score: 0.00\n",
      "Episode: 600, Avg Score: 0.00\n",
      "Episode: 700, Avg Score: 0.00\n",
      "Episode: 800, Avg Score: 0.00\n",
      "Episode: 900, Avg Score: 0.01\n",
      "Episode: 1000, Avg Score: 0.00\n",
      "Episode: 1100, Avg Score: 0.01\n",
      "Episode: 1200, Avg Score: 0.00\n",
      "Episode: 1300, Avg Score: 0.00\n",
      "Episode: 1400, Avg Score: 0.01\n",
      "Episode: 1500, Avg Score: 0.00\n",
      "Episode: 1600, Avg Score: 0.00\n",
      "Episode: 1700, Avg Score: 0.01\n",
      "Episode: 1800, Avg Score: 0.00\n",
      "Episode: 1900, Avg Score: 0.00\n",
      "Episode: 2000, Avg Score: 0.01\n",
      "Episode: 2100, Avg Score: 0.02\n",
      "Episode: 2200, Avg Score: 0.00\n",
      "Episode: 2300, Avg Score: 0.02\n",
      "Episode: 2400, Avg Score: 0.01\n",
      "Episode: 2500, Avg Score: 0.01\n",
      "Episode: 2600, Avg Score: 0.01\n",
      "Episode: 2700, Avg Score: 0.01\n",
      "Episode: 2800, Avg Score: 0.02\n",
      "Episode: 2900, Avg Score: 0.02\n",
      "Episode: 3000, Avg Score: 0.01\n",
      "Episode: 3100, Avg Score: 0.03\n",
      "Episode: 3200, Avg Score: 0.05\n",
      "Episode: 3300, Avg Score: 0.03\n",
      "Episode: 3400, Avg Score: 0.06\n",
      "Episode: 3500, Avg Score: 0.13\n",
      "Episode: 3600, Avg Score: 0.41\n",
      "Episode: 3700, Avg Score: 0.69\n",
      "Episode: 3800, Avg Score: 0.93\n",
      "Episode: 3900, Avg Score: 0.96\n",
      "Episode: 4000, Avg Score: 0.98\n",
      "Episode: 4100, Avg Score: 0.98\n",
      "Episode: 4200, Avg Score: 0.98\n",
      "Episode: 4300, Avg Score: 0.99\n",
      "Episode: 4400, Avg Score: 0.99\n",
      "Episode: 4500, Avg Score: 1.00\n",
      "Episode: 4600, Avg Score: 1.00\n",
      "Episode: 4700, Avg Score: 1.00\n",
      "Episode: 4800, Avg Score: 0.99\n",
      "Episode: 4900, Avg Score: 1.00\n",
      "Episode: 5000, Avg Score: 1.00\n",
      "Episode: 5100, Avg Score: 0.99\n",
      "Episode: 5200, Avg Score: 1.00\n",
      "Episode: 5300, Avg Score: 1.00\n",
      "Episode: 5400, Avg Score: 1.00\n",
      "Episode: 5500, Avg Score: 1.00\n",
      "Episode: 5600, Avg Score: 1.00\n",
      "Episode: 5700, Avg Score: 1.00\n",
      "Episode: 5800, Avg Score: 1.00\n",
      "Episode: 5900, Avg Score: 1.00\n",
      "Episode: 6000, Avg Score: 1.00\n",
      "Episode: 6100, Avg Score: 1.00\n",
      "Episode: 6200, Avg Score: 1.00\n",
      "Episode: 6300, Avg Score: 1.00\n",
      "Episode: 6400, Avg Score: 1.00\n",
      "Episode: 6500, Avg Score: 0.97\n",
      "Episode: 6600, Avg Score: 0.99\n",
      "Episode: 6700, Avg Score: 1.00\n",
      "Episode: 6800, Avg Score: 1.00\n",
      "Episode: 6900, Avg Score: 0.99\n",
      "Episode: 7000, Avg Score: 1.00\n",
      "Episode: 7100, Avg Score: 1.00\n",
      "Episode: 7200, Avg Score: 1.00\n",
      "Episode: 7300, Avg Score: 1.00\n",
      "Episode: 7400, Avg Score: 1.00\n",
      "Episode: 7500, Avg Score: 1.00\n",
      "Episode: 7600, Avg Score: 1.00\n",
      "Episode: 7700, Avg Score: 1.00\n",
      "Episode: 7800, Avg Score: 1.00\n",
      "Episode: 7900, Avg Score: 1.00\n",
      "Episode: 8000, Avg Score: 1.00\n",
      "Episode: 8100, Avg Score: 1.00\n",
      "Episode: 8200, Avg Score: 1.00\n",
      "Episode: 8300, Avg Score: 1.00\n",
      "Episode: 8400, Avg Score: 1.00\n",
      "Episode: 8500, Avg Score: 1.00\n",
      "Episode: 8600, Avg Score: 1.00\n",
      "Episode: 8700, Avg Score: 1.00\n",
      "Episode: 8800, Avg Score: 1.00\n",
      "Episode: 8900, Avg Score: 1.00\n",
      "Episode: 9000, Avg Score: 1.00\n",
      "Episode: 9100, Avg Score: 1.00\n",
      "Episode: 9200, Avg Score: 1.00\n",
      "Episode: 9300, Avg Score: 1.00\n",
      "Episode: 9400, Avg Score: 1.00\n",
      "Episode: 9500, Avg Score: 1.00\n",
      "Episode: 9600, Avg Score: 1.00\n",
      "Episode: 9700, Avg Score: 1.00\n",
      "Episode: 9800, Avg Score: 1.00\n",
      "Episode: 9900, Avg Score: 1.00\n",
      "Episode: 10000, Avg Score: 1.00\n",
      "Episode: 10100, Avg Score: 1.00\n",
      "Episode: 10200, Avg Score: 1.00\n",
      "Episode: 10300, Avg Score: 1.00\n",
      "Episode: 10400, Avg Score: 1.00\n",
      "Episode: 10500, Avg Score: 1.00\n",
      "Episode: 10600, Avg Score: 1.00\n",
      "Episode: 10700, Avg Score: 1.00\n",
      "Episode: 10800, Avg Score: 1.00\n",
      "Episode: 10900, Avg Score: 1.00\n",
      "Episode: 11000, Avg Score: 1.00\n",
      "Episode: 11100, Avg Score: 1.00\n",
      "Episode: 11200, Avg Score: 1.00\n",
      "Episode: 11300, Avg Score: 1.00\n",
      "Episode: 11400, Avg Score: 1.00\n",
      "Episode: 11500, Avg Score: 1.00\n",
      "Episode: 11600, Avg Score: 1.00\n",
      "Episode: 11700, Avg Score: 1.00\n",
      "Episode: 11800, Avg Score: 1.00\n",
      "Episode: 11900, Avg Score: 1.00\n",
      "Episode: 12000, Avg Score: 1.00\n",
      "Episode: 12100, Avg Score: 1.00\n",
      "Episode: 12200, Avg Score: 1.00\n",
      "Episode: 12300, Avg Score: 1.00\n",
      "Episode: 12400, Avg Score: 1.00\n",
      "Episode: 12500, Avg Score: 1.00\n",
      "Episode: 12600, Avg Score: 1.00\n",
      "Episode: 12700, Avg Score: 1.00\n",
      "Episode: 12800, Avg Score: 1.00\n",
      "Episode: 12900, Avg Score: 1.00\n",
      "Episode: 13000, Avg Score: 1.00\n",
      "Episode: 13100, Avg Score: 1.00\n",
      "Episode: 13200, Avg Score: 1.00\n",
      "Episode: 13300, Avg Score: 1.00\n",
      "Episode: 13400, Avg Score: 1.00\n",
      "Episode: 13500, Avg Score: 1.00\n",
      "Episode: 13600, Avg Score: 1.00\n",
      "Episode: 13700, Avg Score: 1.00\n",
      "Episode: 13800, Avg Score: 1.00\n",
      "Episode: 13900, Avg Score: 0.98\n",
      "Episode: 14000, Avg Score: 1.00\n",
      "Episode: 14100, Avg Score: 1.00\n",
      "Episode: 14200, Avg Score: 1.00\n",
      "Episode: 14300, Avg Score: 1.00\n",
      "Episode: 14400, Avg Score: 1.00\n",
      "Episode: 14500, Avg Score: 1.00\n",
      "Episode: 14600, Avg Score: 1.00\n",
      "Episode: 14700, Avg Score: 1.00\n",
      "Episode: 14800, Avg Score: 1.00\n",
      "Episode: 14900, Avg Score: 1.00\n",
      "Episode: 15000, Avg Score: 1.00\n",
      "Episode: 15100, Avg Score: 1.00\n",
      "Episode: 15200, Avg Score: 1.00\n",
      "Episode: 15300, Avg Score: 1.00\n",
      "Episode: 15400, Avg Score: 1.00\n",
      "Episode: 15500, Avg Score: 1.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trained_model = \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mtrain_agent\u001b[39m\u001b[34m(env)\u001b[39m\n\u001b[32m     21\u001b[39m     prob = model.pi(s)\n\u001b[32m     22\u001b[39m m = Categorical(prob)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m a = \u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.item()\n\u001b[32m     24\u001b[39m s_prime, r, terminated, truncated, _ = env.step(a)\n\u001b[32m     25\u001b[39m done = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rl/lib/python3.12/site-packages/torch/distributions/categorical.py:134\u001b[39m, in \u001b[36mCategorical.sample\u001b[39m\u001b[34m(self, sample_shape)\u001b[39m\n\u001b[32m    132\u001b[39m     sample_shape = torch.Size(sample_shape)\n\u001b[32m    133\u001b[39m probs_2d = \u001b[38;5;28mself\u001b[39m.probs.reshape(-\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m._num_events)\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m samples_2d = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_shape\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m.T\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m samples_2d.reshape(\u001b[38;5;28mself\u001b[39m._extended_shape(sample_shape))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trained_model = train_agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access runs/: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls runs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf runs/A2C_8x8/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trained_model.state_dict(), \"models/frozenlake_a2c.pth\")\n",
    "print(\"Model saved as frozenlake_a2c.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = ActorCritic(state_dim, action_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from frozenlake_actor_critic.pth\n"
     ]
    }
   ],
   "source": [
    "trained_model.load_state_dict(torch.load(\"models/frozenlake_a2c.pth\", map_location=device))\n",
    "trained_model.eval()  # Set to evaluation mode if only using for inference\n",
    "print(\"Model loaded from frozenlake_a2c.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
