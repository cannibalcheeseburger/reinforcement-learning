{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else  \n",
    "    \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0002\n",
    "gamma = 0.98\n",
    "n_rollout = 30\n",
    "map_size = 8\n",
    "env = gym.make('FrozenLake-v1',map_name = f'{map_size}x{map_size}', is_slippery=False)\n",
    "state_dim = env.observation_space.n\n",
    "action_dim = env.action_space.n\n",
    "max_episodes = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        # Actor network (Ï€)\n",
    "        self.pi_fc1 = nn.Linear(state_dim, 128)\n",
    "        self.pi_fc2 = nn.Linear(128, action_dim)\n",
    "        \n",
    "        # Critic network (V)\n",
    "        self.v_fc1 = nn.Linear(state_dim, 128)\n",
    "        self.v_fc2 = nn.Linear(128, 1)\n",
    "        \n",
    "        # Separate optimizers\n",
    "        self.actor_optim = optim.Adam(self.actor_parameters(), lr=0.001)\n",
    "        self.critic_optim = optim.Adam(self.critic_parameters(), lr=0.002)\n",
    "        self.data = []\n",
    "    \n",
    "    def actor_parameters(self):\n",
    "        return list(self.pi_fc1.parameters()) + list(self.pi_fc2.parameters())\n",
    "    \n",
    "    def critic_parameters(self):\n",
    "        return list(self.v_fc1.parameters()) + list(self.v_fc2.parameters())\n",
    "        \n",
    "    def pi(self, x, softmax_dim=0):\n",
    "        x = F.relu(self.pi_fc1(x))\n",
    "        x = self.pi_fc2(x)\n",
    "        return F.softmax(x, dim=softmax_dim)\n",
    "    \n",
    "    def v(self, x):\n",
    "        x = F.relu(self.v_fc1(x))\n",
    "        return self.v_fc2(x)\n",
    "    \n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    def make_batch(self):\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_lst = [], [], [], [], []\n",
    "        for transition in self.data:\n",
    "            s, a, r, s_prime, done = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r/100.0])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_lst.append([0.0] if done else [1.0])\n",
    "            \n",
    "        return (torch.tensor(s_lst, dtype=torch.float, device=device),\n",
    "                torch.tensor(a_lst, device=device),\n",
    "                torch.tensor(r_lst, dtype=torch.float, device=device),\n",
    "                torch.tensor(s_prime_lst, dtype=torch.float, device=device),\n",
    "                torch.tensor(done_lst, dtype=torch.float, device=device))\n",
    "  \n",
    "    def train_net(self):\n",
    "        s, a, r, s_prime, done = self.make_batch()\n",
    "        td_target = r + gamma * self.v(s_prime) * done        \n",
    "        pi = self.pi(s, softmax_dim=1)\n",
    "        pi_a = pi.gather(1, a)\n",
    "        policy_loss = -torch.log(pi_a) * td_target.detach() \n",
    "        value_loss = F.mse_loss(self.v(s), td_target.detach())\n",
    "        loss = policy_loss + value_loss\n",
    "        self.actor_optim.zero_grad()\n",
    "        self.critic_optim.zero_grad()\n",
    "\n",
    "        policy_loss.mean().backward()\n",
    "        value_loss.backward()\n",
    "\n",
    "        self.actor_optim.step()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        self.data = []\n",
    "        return policy_loss.mean().detach(),value_loss.detach(),loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(state, state_dim):\n",
    "    vec = np.zeros(state_dim)\n",
    "    vec[state] = 1.0\n",
    "    return torch.tensor(vec, dtype=torch.float, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env):\n",
    "    state_dim = env.observation_space.n\n",
    "    action_dim = env.action_space.n\n",
    "    writer = SummaryWriter(log_dir=f'runs/ActorCritic_{map_size}x{map_size}')\n",
    "\n",
    "    model = ActorCritic(state_dim, action_dim)\n",
    "    model.to(device)\n",
    "    print_interval = 100\n",
    "    score = 0.0\n",
    "    steps = 0\n",
    "    for n_epi in range(max_episodes):\n",
    "        s, _ = env.reset()\n",
    "        s = one_hot(s, state_dim)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            for t in range(n_rollout):\n",
    "                steps +=1\n",
    "                with torch.no_grad():\n",
    "                    prob = model.pi(s)\n",
    "                m = Categorical(prob)\n",
    "                a = m.sample().item()\n",
    "                s_prime, r, terminated, truncated, _ = env.step(a)\n",
    "                done = terminated or truncated\n",
    "                s_prime_oh = one_hot(s_prime, state_dim)\n",
    "                # not done??\n",
    "                model.put_data((s.cpu().numpy(), a, r, s_prime_oh.cpu().numpy(), done))\n",
    "                s = s_prime_oh\n",
    "                score += r\n",
    "                if done:\n",
    "                    break\n",
    "        actor_loss,critic_loss,loss = model.train_net()\n",
    "\n",
    "        if n_epi % print_interval == 0 and n_epi != 0:\n",
    "            avg_score = score / print_interval\n",
    "            print(f\"Episode: {n_epi}, Avg Score: {avg_score:.4f}, Critic Loss:{critic_loss:.6f}\")\n",
    "            # Log to TensorBoard\n",
    "            writer.add_scalar('Return', avg_score, steps)\n",
    "            # writer.add_scalar('Actor Loss', actor_loss, steps)\n",
    "            writer.add_scalar('Critic Loss', critic_loss.item(), steps)\n",
    "            score = 0.0\n",
    "    env.close()\n",
    "    writer.close()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(model):\n",
    "    env = gym.make('FrozenLake-v1', is_slippery=False, map_name = f'{map_size}x{map_size}', render_mode='human')\n",
    "    state_dim = env.observation_space.n\n",
    "    s, _ = env.reset()\n",
    "    s = one_hot(s, state_dim)\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            prob = model.pi(s)\n",
    "        a = torch.argmax(prob).item()\n",
    "        s_prime, r, terminated, truncated, _ = env.step(a)\n",
    "        done = terminated or truncated\n",
    "        s = one_hot(s_prime, state_dim)\n",
    "        env.render()\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.508615.pbshpc/ipykernel_28323/2785047123.py:45: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  return (torch.tensor(s_lst, dtype=torch.float, device=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100, Avg Score: 0.0000, Critic Loss:0.000007\n",
      "Episode: 200, Avg Score: 0.0000, Critic Loss:0.000003\n",
      "Episode: 300, Avg Score: 0.0100, Critic Loss:0.000002\n",
      "Episode: 400, Avg Score: 0.0100, Critic Loss:0.000004\n",
      "Episode: 500, Avg Score: 0.0000, Critic Loss:0.000002\n",
      "Episode: 600, Avg Score: 0.0100, Critic Loss:0.000001\n",
      "Episode: 700, Avg Score: 0.0100, Critic Loss:0.000001\n",
      "Episode: 800, Avg Score: 0.0000, Critic Loss:0.000003\n",
      "Episode: 900, Avg Score: 0.0000, Critic Loss:0.000000\n",
      "Episode: 1000, Avg Score: 0.0000, Critic Loss:0.000002\n",
      "Episode: 1100, Avg Score: 0.0000, Critic Loss:0.000004\n",
      "Episode: 1200, Avg Score: 0.0200, Critic Loss:0.000001\n",
      "Episode: 1300, Avg Score: 0.0000, Critic Loss:0.000000\n",
      "Episode: 1400, Avg Score: 0.0000, Critic Loss:0.000000\n",
      "Episode: 1500, Avg Score: 0.0300, Critic Loss:0.000000\n",
      "Episode: 1600, Avg Score: 0.0000, Critic Loss:0.000000\n",
      "Episode: 1700, Avg Score: 0.0100, Critic Loss:0.000000\n",
      "Episode: 1800, Avg Score: 0.0100, Critic Loss:0.000000\n",
      "Episode: 1900, Avg Score: 0.0200, Critic Loss:0.000000\n",
      "Episode: 2000, Avg Score: 0.0400, Critic Loss:0.000001\n",
      "Episode: 2100, Avg Score: 0.0500, Critic Loss:0.000000\n",
      "Episode: 2200, Avg Score: 0.0400, Critic Loss:0.000013\n",
      "Episode: 2300, Avg Score: 0.1300, Critic Loss:0.000000\n",
      "Episode: 2400, Avg Score: 0.0500, Critic Loss:0.000001\n",
      "Episode: 2500, Avg Score: 0.0400, Critic Loss:0.000000\n",
      "Episode: 2600, Avg Score: 0.1200, Critic Loss:0.000002\n",
      "Episode: 2700, Avg Score: 0.3000, Critic Loss:0.000003\n",
      "Episode: 2800, Avg Score: 0.4600, Critic Loss:0.000011\n",
      "Episode: 2900, Avg Score: 0.4400, Critic Loss:0.000004\n",
      "Episode: 3000, Avg Score: 0.4800, Critic Loss:0.000001\n",
      "Episode: 3100, Avg Score: 0.7900, Critic Loss:0.000004\n",
      "Episode: 3200, Avg Score: 0.9000, Critic Loss:0.000001\n",
      "Episode: 3300, Avg Score: 0.9300, Critic Loss:0.000001\n",
      "Episode: 3400, Avg Score: 0.9500, Critic Loss:0.000000\n",
      "Episode: 3500, Avg Score: 0.9600, Critic Loss:0.000000\n",
      "Episode: 3600, Avg Score: 0.9900, Critic Loss:0.000000\n",
      "Episode: 3700, Avg Score: 0.9600, Critic Loss:0.000000\n",
      "Episode: 3800, Avg Score: 0.9800, Critic Loss:0.000000\n",
      "Episode: 3900, Avg Score: 0.9700, Critic Loss:0.000000\n",
      "Episode: 4000, Avg Score: 0.9800, Critic Loss:0.000000\n",
      "Episode: 4100, Avg Score: 0.9800, Critic Loss:0.000000\n",
      "Episode: 4200, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 4300, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 4400, Avg Score: 0.9900, Critic Loss:0.000000\n",
      "Episode: 4500, Avg Score: 0.9900, Critic Loss:0.000000\n",
      "Episode: 4600, Avg Score: 0.9800, Critic Loss:0.000001\n",
      "Episode: 4700, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 4800, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 4900, Avg Score: 0.9900, Critic Loss:0.000000\n",
      "Episode: 5000, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 5100, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 5200, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 5300, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 5400, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 5500, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 5600, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 5700, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 5800, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 5900, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 6000, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 6100, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 6200, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 6300, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 6400, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 6500, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 6600, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 6700, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 6800, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 6900, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 7000, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 7100, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 7200, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 7300, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 7400, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 7500, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 7600, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 7700, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 7800, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 7900, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 8000, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 8100, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 8200, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 8300, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 8400, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 8500, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 8600, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 8700, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 8800, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 8900, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 9000, Avg Score: 0.9500, Critic Loss:0.000000\n",
      "Episode: 9100, Avg Score: 0.9800, Critic Loss:0.000001\n",
      "Episode: 9200, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 9300, Avg Score: 0.9800, Critic Loss:0.000000\n",
      "Episode: 9400, Avg Score: 0.9900, Critic Loss:0.000001\n",
      "Episode: 9500, Avg Score: 0.9900, Critic Loss:0.000000\n",
      "Episode: 9600, Avg Score: 0.9900, Critic Loss:0.000000\n",
      "Episode: 9700, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 9800, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 9900, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 10000, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 10100, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 10200, Avg Score: 0.9800, Critic Loss:0.000000\n",
      "Episode: 10300, Avg Score: 0.9500, Critic Loss:0.000005\n",
      "Episode: 10400, Avg Score: 0.9400, Critic Loss:0.000000\n",
      "Episode: 10500, Avg Score: 0.9300, Critic Loss:0.000002\n",
      "Episode: 10600, Avg Score: 0.9700, Critic Loss:0.000000\n",
      "Episode: 10700, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 10800, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 10900, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 11000, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 11100, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 11200, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 11300, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 11400, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 11500, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 11600, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 11700, Avg Score: 0.9900, Critic Loss:0.000000\n",
      "Episode: 11800, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 11900, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 12000, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 12100, Avg Score: 0.9900, Critic Loss:0.000000\n",
      "Episode: 12200, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 12300, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 12400, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 12500, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 12600, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 12700, Avg Score: 0.9900, Critic Loss:0.000000\n",
      "Episode: 12800, Avg Score: 0.9800, Critic Loss:0.000006\n",
      "Episode: 12900, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 13000, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 13100, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 13200, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 13300, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 13400, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 13500, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 13600, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 13700, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 13800, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 13900, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 14000, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 14100, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 14200, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 14300, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 14400, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 14500, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 14600, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 14700, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 14800, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 14900, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 15000, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 15100, Avg Score: 1.0000, Critic Loss:0.000000\n",
      "Episode: 15200, Avg Score: 1.0000, Critic Loss:0.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trained_model = \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mtrain_agent\u001b[39m\u001b[34m(env)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     20\u001b[39m     prob = model.pi(s)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m m = \u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m a = m.sample().item()\n\u001b[32m     23\u001b[39m s_prime, r, terminated, truncated, _ = env.step(a)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rl/lib/python3.12/site-packages/torch/distributions/categorical.py:72\u001b[39m, in \u001b[36mCategorical.__init__\u001b[39m\u001b[34m(self, probs, logits, validate_args)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28mself\u001b[39m._num_events = \u001b[38;5;28mself\u001b[39m._param.size()[-\u001b[32m1\u001b[39m]\n\u001b[32m     69\u001b[39m batch_shape = (\n\u001b[32m     70\u001b[39m     \u001b[38;5;28mself\u001b[39m._param.size()[:-\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._param.ndimension() > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch.Size()\n\u001b[32m     71\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rl/lib/python3.12/site-packages/torch/distributions/distribution.py:69\u001b[39m, in \u001b[36mDistribution.__init__\u001b[39m\u001b[34m(self, batch_shape, event_shape, validate_args)\u001b[39m\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# skip checking lazily-constructed args\u001b[39;00m\n\u001b[32m     68\u001b[39m value = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m valid = \u001b[43mconstraint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid.all():\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     72\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value.shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     77\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rl/lib/python3.12/site-packages/torch/distributions/constraints.py:464\u001b[39m, in \u001b[36m_Simplex.check\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.all(value >= \u001b[32m0\u001b[39m, dim=-\u001b[32m1\u001b[39m) & (\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m < \u001b[32m1e-6\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trained_model = train_agent(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor_critic.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf runs/ActorCritic_8x8/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trained_model.state_dict(), \"models/frozenlake_actor_critic.pth\")\n",
    "print(\"Model saved as frozenlake_actor_critic.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = ActorCritic(state_dim, action_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from frozenlake_actor_critic.pth\n"
     ]
    }
   ],
   "source": [
    "trained_model.load_state_dict(torch.load(\"models/frozenlake_actor_critic.pth\", map_location=device))\n",
    "trained_model.eval()  # Set to evaluation mode if only using for inference\n",
    "print(\"Model loaded from frozenlake_actor_critic.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent(trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
